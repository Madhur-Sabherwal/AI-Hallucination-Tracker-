

# AI Hallucination Tracker (Client-Side)

## ğŸ“Œ Overview

The **AI Hallucination Tracker** is a **single-file HTML web application** that runs entirely in your browser â€” no server, no backend, no data leakage.
It compares AI model outputs against ground truth datasets to calculate accuracy metrics, detect hallucinations, and generate visual leaderboards.

Built with a **warm gradient theme** and **animated ember effects**, itâ€™s both functional and visually engaging.

---

## âœ¨ Features

* **Client-Side Processing** â€“ Your data never leaves your machine.
* **CSV Upload** â€“ Load your own *Ground Truth* and *Predictions* files.
* **Multiple Metrics** â€“ Exact Match, Token-F1, ROUGE-L (Longest Common Subsequence).
* **Hallucination Detection** â€“ Flags long, incorrect responses.
* **Visual Leaderboard** â€“ Per-model performance at a glance.
* **Error Buckets** â€“ See where each model fails most.
* **Row-Level Explorer** â€“ Drill down into specific cases.
* **CSV Export** â€“ Download processed results for reporting.
* **Stylized UI** â€“ Warm color palette with animated embers.

---

## ğŸ“‚ File Structure

```
index.html   â†’ Main single-page app
README.md    â†’ This documentation
```

---

## ğŸ›  How to Use Locally

1. **Download** `index.html` from this repo.
2. Double-click to open it in your browser.
3. Click **Upload Ground Truth CSV** and **Upload Predictions CSV**.

   * Ground Truth CSV format:

     ```
     id,question,answer
     1,What is the capital of France?,Paris
     ```
   * Predictions CSV format:

     ```
     id,model,output
     1,Model A,Paris
     ```
4. Review metrics, explore flagged hallucinations, and export results.

---

## ğŸŒ Deploy to GitHub Pages

1. Create a **public repository** on GitHub.
2. Upload `index.html` to the **root** of your repo.
3. Commit & push changes.
4. Go to **Settings â†’ Pages**.
5. Under *Branch*, select `main` (or your default branch) and `/ (root)`.
6. Save and wait \~1â€“2 minutes for deployment.
7. Access your live app via the GitHub Pages URL provided.

---

## ğŸ“Š Metrics Explained

* **Exact Match (EM)** â€“ 1 if output matches exactly, 0 otherwise.
* **Token-F1** â€“ Harmonic mean of precision and recall over token overlap.
* **ROUGE-L** â€“ Measures longest common subsequence between output and ground truth.
* **Composite Score** â€“ Weighted combination of the above metrics.

---

## âš™ï¸ Customization

* Edit **metric weights** in the JavaScript section to adjust scoring.
* Change **hallucination threshold** to tune sensitivity.
* Modify **CSS variables** to tweak colors, embers, or background.

---

## ğŸ“œ License

This project is released under the MIT License â€” youâ€™re free to use, modify, and distribute.
